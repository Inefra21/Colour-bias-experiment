{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdMwNLqJL0ELNJdzkxdxgf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Inefra21/Colour-bias-experiment/blob/main/Test_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing models"
      ],
      "metadata": {
        "id": "svLvWBwA0RNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing packages"
      ],
      "metadata": {
        "id": "WfzDqeIh0b0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "edMcb9uR9FWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data and base model"
      ],
      "metadata": {
        "id": "80wPQyDh02DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount the drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UAhmAlH18rYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcLdnap24TzQ"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "# Change file path\n",
        "model = tf.keras.models.load_model(\n",
        "  '/content/drive/MyDrive/model_90.keras',\n",
        "  custom_objects={'KerasLayer': hub.KerasLayer})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory to test images\n",
        "test_lcat_dir = '/content/drive/MyDrive/test_cats/light'\n",
        "test_dcat_dir = '/content/drive/MyDrive/test_cats/dark'\n",
        "test_ldog_dir = '/content/drive/MyDrive/test_dogs/light'\n",
        "test_ddog_dir = '/content/drive/MyDrive/test_dogs/dark'\n",
        "test_mcat_dir = '/content/drive/MyDrive/test_cats/mixed'\n",
        "test_mdog_dir = '/content/drive/MyDrive/test_dogs/mixed'"
      ],
      "metadata": {
        "id": "I6bxMYLd5x3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the data"
      ],
      "metadata": {
        "id": "gf7VV61s5Bov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the images in each folder\n",
        "# You must add the a string to the end of the directory because of the structure of the dataset\n",
        "num_lcat = len(os.listdir(test_lcat_dir + '/light'))\n",
        "num_dcat = len(os.listdir(test_dcat_dir + '/dark'))\n",
        "num_ldog = len(os.listdir(test_ldog_dir + '/light'))\n",
        "num_ddog = len(os.listdir(test_ddog_dir + '/dark'))\n",
        "\n",
        "num_mcat = len(os.listdir(test_mcat_dir + '/mixed'))\n",
        "num_mdog = len(os.listdir(test_mdog_dir + '/mixed'))\n",
        "\n",
        "total_cats = num_lcat + num_dcat + num_mcat\n",
        "total_dogs = num_ldog + num_ddog + num_mdog"
      ],
      "metadata": {
        "id": "E22-8Nsh5J3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('total test cat images:', total_cats)\n",
        "print('total test dog images:', total_dogs)"
      ],
      "metadata": {
        "id": "O2CWfw2Q6SqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting model parameters"
      ],
      "metadata": {
        "id": "MXslIMj508cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The image generator will produce batches of 50 images of shape 150 pixels by 150 pixels\n",
        "IMG_SHAPE = 150\n",
        "BATCH_SIZE1 = 50\n",
        "BATCH_SIZE2 = 500"
      ],
      "metadata": {
        "id": "S_2NNXvi9xil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating batched datasets"
      ],
      "metadata": {
        "id": "6a-ivWUQ1bKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_gen_test = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Image generator with light cats\n",
        "test_lcat_gen = image_gen_test.flow_from_directory(batch_size=BATCH_SIZE1,\n",
        "                                                 directory=test_lcat_dir,\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "                                                 class_mode='sparse')\n",
        "\n",
        "# Image generator with dark cats\n",
        "test_dcat_gen = image_gen_test.flow_from_directory(batch_size=BATCH_SIZE1,\n",
        "                                                 directory=test_dcat_dir,\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "                                                 class_mode='sparse')\n",
        "\n",
        "# Image generator with light dogs\n",
        "test_ldog_gen = image_gen_test.flow_from_directory(batch_size=BATCH_SIZE1,\n",
        "                                                 directory=test_ldog_dir,\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "                                                 class_mode='sparse')\n",
        "\n",
        "# Image generator with dark dogs\n",
        "test_ddog_gen = image_gen_test.flow_from_directory(batch_size=BATCH_SIZE1,\n",
        "                                                 directory=test_ddog_dir,\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "                                                 class_mode='sparse')"
      ],
      "metadata": {
        "id": "25XFu7uJ50xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size is set to 500 as there are more images\n",
        "# Image generator with mixed cat images\n",
        "test_mcat_gen = image_gen_test.flow_from_directory(batch_size=BATCH_SIZE2,\n",
        "                                                 directory=test_mcat_dir,\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "                                                 class_mode='sparse')\n",
        "\n",
        "# Image generator with mixed dog images\n",
        "test_mdog_gen = image_gen_test.flow_from_directory(batch_size=BATCH_SIZE2,\n",
        "                                                 directory=test_mdog_dir,\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "                                                 class_mode='sparse')"
      ],
      "metadata": {
        "id": "CpZnGEV1oF14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the models"
      ],
      "metadata": {
        "id": "ihhvN3p01nHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store results of predictions\n",
        "lcat_predicted = tf.squeeze(model.predict(test_lcat_gen)).numpy()\n",
        "dcat_predicted = tf.squeeze(model.predict(test_dcat_gen)).numpy()\n",
        "ldog_predicted = tf.squeeze(model.predict(test_ldog_gen)).numpy()\n",
        "ddog_predicted = tf.squeeze(model.predict(test_ddog_gen)).numpy()\n",
        "\n",
        "# Calculate the label with the highest probability for each prediction\n",
        "lcat_predicted_lb = np.argmax(lcat_predicted, axis=-1)\n",
        "dcat_predicted_lb = np.argmax(dcat_predicted, axis=-1)\n",
        "ldog_predicted_lb = np.argmax(ldog_predicted, axis=-1)\n",
        "ddog_predicted_lb = np.argmax(ddog_predicted, axis=-1)"
      ],
      "metadata": {
        "id": "uPYx35FNPj3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create variables to count the number of mistakes\n",
        "lcat_mistakes = 0\n",
        "dcat_mistakes = 0\n",
        "ldog_mistakes = 0\n",
        "ddog_mistakes = 0\n",
        "\n",
        "# Count number of mistakes for each category\n",
        "for prediction in range(num_lcat):\n",
        "  if lcat_predicted_lb[prediction] == 1:\n",
        "    lcat_mistakes += 1\n",
        "\n",
        "for prediction in range(num_dcat):\n",
        "  if dcat_predicted_lb[prediction] == 1:\n",
        "    dcat_mistakes += 1\n",
        "\n",
        "for prediction in range(num_ldog):\n",
        "  if ldog_predicted_lb[prediction] == 0:\n",
        "    ldog_mistakes += 1\n",
        "\n",
        "for prediction in range(num_ddog):\n",
        "  if ddog_predicted_lb[prediction] == 0:\n",
        "    ddog_mistakes += 1\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy light cats:\", 100 - (lcat_mistakes*100/num_lcat), \"%\")\n",
        "print(\"Accuracy dark cats:\", 100 - (dcat_mistakes*100/num_dcat), \"%\")\n",
        "print(\"Accuracy light dogs:\", 100 - (ldog_mistakes*100/num_ldog), \"%\")\n",
        "print(\"Accuracy dark dogs:\", 100 - (ddog_mistakes*100/num_ddog), \"%\")"
      ],
      "metadata": {
        "id": "VovcdKJ654W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict labels for mixed cat images\n",
        "# Save the images for visualising later\n",
        "mcat_images, mcat_labels = next(iter(test_mcat_gen))\n",
        "mcat_predicted = tf.squeeze(model.predict(mcat_images)).numpy()\n",
        "mcat_predicted_lb = np.argmax(mcat_predicted, axis=-1)\n",
        "\n",
        "# Predict labels for mixed dog images\n",
        "# Save the images for visualising later\n",
        "mdog_images, mdog_labels = next(iter(test_mdog_gen))\n",
        "mdog_predicted = tf.squeeze(model.predict(mdog_images)).numpy()\n",
        "mdog_predicted_lb = np.argmax(mdog_predicted, axis=-1)"
      ],
      "metadata": {
        "id": "oxmZ9Anp_XMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create array to store the misclassified images\n",
        "mcat_mistakes = []\n",
        "mdog_mistakes = []\n",
        "\n",
        "for prediction in range(num_mcat):\n",
        "  if mcat_predicted_lb[prediction] == 1:\n",
        "    mcat_mistakes.append(mcat_images[prediction])\n",
        "\n",
        "for prediction in range(num_mdog):\n",
        "  if mdog_predicted_lb[prediction] == 0:\n",
        "    mdog_mistakes.append(mdog_images[prediction])\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy on cats:\", 100 - (len(mcat_mistakes)*100/num_mcat), \"%\")\n",
        "print(\"Accuracy on dogs:\", 100 - (len(mdog_mistakes)*100/num_mdog), \"%\")"
      ],
      "metadata": {
        "id": "Z-A9VyLsn8Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing some of the misclassified images"
      ],
      "metadata": {
        "id": "BjRhjzrwEZvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 1\n",
        "\n",
        "# Show the cat images which were classified as dogs\n",
        "plt.figure(figsize=(10, 9))\n",
        "for n, img in enumerate(mcat_mistakes):\n",
        "  if n < 30:\n",
        "    plt.subplot(6, 5, counter)\n",
        "    plt.subplots_adjust(hspace = 0.3)\n",
        "    plt.imshow(mcat_images[n])\n",
        "    plt.axis('off')\n",
        "    counter += 1\n",
        "_ = plt.suptitle(\"Misclassified cat images\")"
      ],
      "metadata": {
        "id": "dJg7EyBy-Plb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 1\n",
        "\n",
        "# Show the dog images which were classified as cats\n",
        "plt.figure(figsize=(10, 9))\n",
        "for n, img in enumerate(mdog_mistakes):\n",
        "  if n < 30:\n",
        "  plt.subplot(6, 5, counter)\n",
        "  plt.subplots_adjust(hspace = 0.3)\n",
        "  plt.imshow(mdog_images[n])\n",
        "  plt.axis('off')\n",
        "  counter += 1\n",
        "_ = plt.suptitle(\"Misclassified dog images\")"
      ],
      "metadata": {
        "id": "wl0zOwyDAcEW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}